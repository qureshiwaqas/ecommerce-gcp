{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cffcce23-7524-4dc0-a2af-229b6aa92173",
   "metadata": {},
   "source": [
    "# **Cluster Configuration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9f172e-9f2e-4ef6-9313-e1aedb1df2a0",
   "metadata": {},
   "source": [
    "1. **Cluster Resources:**\n",
    "- **Master:** n2-standard-4 (4 vCPUs, 16 GB RAM, 32GB disk)\n",
    "- **Workers (2x):** n2-standard-4 (4 vCPUs, 16 GB RAM, 64GB disk each)\n",
    "- **Total:** 8 worker vCPUs, ~32 GB RAM (excluding master node)\n",
    "2. **Dataproc Features Disabled**:\n",
    "- No **autoscaling, Metastore, advanced execution layer, advanced optimizations**\n",
    "- **Storage**: pd-balanced (no SSDs, so I/O optimization is crucial)\n",
    "- **Networking**: Internal IP **enabled**\n",
    "3. Optimization Strategy:\n",
    "- Tune **shuffle partitions, broadcast join threshold, and storage persistence**\n",
    "- Adjust **parallelism** based on **2 workers x 4 cores**\n",
    "- Avoid **excessive caching** due to **disk-based storage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b1e6cd6-a4c4-413e-8d10-26cc2e4c1210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bb4f62-3472-4b63-92fd-e8ca340b1213",
   "metadata": {},
   "source": [
    "## **Some partitions configs and their usages**\n",
    "- spark.executor.memory: using 6GB out of 16 keeping some for sys operations\n",
    "- spark.executor.cores: using max cores of our setup (i.e. 4)\n",
    "- spark.executor.instances: using max instance of our setup (i.e. number of worker nodes)\n",
    "- spark.driver.memory: driver memory\n",
    "- spark.driver.maxResultSize: max size of a partition\n",
    "- spark.sql.shuffle.partitions: no of partitions during joins/aggregations\n",
    "- spark.default.parallelism: no of paritions in RDD returned by transformations\n",
    "- spark.sql.adaptive.enabled: reoptimzie queries during execution\n",
    "- spark.sql.adaptive.coalescePartition.enabled: coalesces continuous paritions\n",
    "- spark.sql.autoBroadcastJoinThreshold: max size send over a broadcast\n",
    "- spark.sql.files.maxPartitionBytes: max size in a partition when reading\n",
    "- spark.sql.files.openCostInBytes: cost to open a file (reads that many bytes)\n",
    "- spark.memory.fraction: fraction in memory\n",
    "- spark.memory.storageFraction: fraction in storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc23bd1b-47bc-4b8b-923e-2ce448a4b37a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/25 23:03:30 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    ".appName('Olist Ecommerce Optimized')\\\n",
    ".config('spark.executor.memory', '6g')\\\n",
    ".config('spark.executor.cores', '4')\\\n",
    ".config('spark.executor.instances', '2')\\\n",
    ".config('spark.driver.memory', '4g')\\\n",
    ".config('spark.driver.maxResultSize', '2g')\\\n",
    ".config('spark.sql.shuffle.partitions', '64')\\\n",
    ".config('spark.default.parallelism', '64')\\\n",
    ".config('spark.sql.adaptive.enabled', 'true')\\\n",
    ".config('spark.sql.adaptive.coalescePartition.enabled', 'true')\\\n",
    ".config('spark.sql.autoBroadcastJoinThreshold', 20*1024*1024)\\\n",
    ".config('spark.sql.files.maxPartitionBytes', '64MB')\\\n",
    ".config('spark.sql.files.openCostInBytes', '2MB')\\\n",
    ".config('spark.memory.fraction', '0.8')\\\n",
    ".config('spark.memory.storageFraction', '0.2')\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1faa418-8fdd-4f48-84d2-ab6b287e7ecc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hdfs_path = '/data/olist/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3d4bf5e-67fc-41f6-94b4-ee7e586e16dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "customers_df = spark.read.csv(hdfs_path + 'olist_customers_dataset.csv', inferSchema='true', header='true')\n",
    "geolocation_df = spark.read.csv(hdfs_path + 'olist_geolocation_dataset.csv', inferSchema='true', header='true')\n",
    "order_items_df = spark.read.csv(hdfs_path + 'olist_order_items_dataset.csv', inferSchema='true', header='true')\n",
    "order_payments_df = spark.read.csv(hdfs_path + 'olist_order_payments_dataset.csv', inferSchema='true', header='true')\n",
    "order_reviews_df = spark.read.csv(hdfs_path + 'olist_order_reviews_dataset.csv', inferSchema='true', header='true')\n",
    "orders_df = spark.read.csv(hdfs_path + 'olist_orders_dataset.csv', inferSchema='true', header='true')\n",
    "products_df = spark.read.csv(hdfs_path + 'olist_products_dataset.csv', inferSchema='true', header='true')\n",
    "sellers_df = spark.read.csv(hdfs_path + 'olist_sellers_dataset.csv', inferSchema='true', header='true')\n",
    "product_category_translation_df = spark.read.csv(hdfs_path + 'product_category_name_translation.csv', inferSchema='true', header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7a9f10b-67a7-44e1-beb4-b9401369530e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "complete_df = spark.read.parquet('/data/olist/olist-merged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa3be48-adba-44fb-9df8-aaccefb95209",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "complete_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a211bf3-0c43-4a18-9aa1-1f1a938c8217",
   "metadata": {},
   "source": [
    "## Optimized Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6139302-1602-4167-89fe-546d9b07e84b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Broadcast Join**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9279f962-c561-4549-855a-69c0a39329d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "customers_broadcast_df = broadcast(customers_df)\n",
    "optimized_broadcast_df = complete_df.join(customers_broadcast_df, 'customer_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f501b82a-ae0a-4b0b-8e3b-c94891660354",
   "metadata": {},
   "source": [
    "### **Sort & Merge Join**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c707639a-aca2-4b86-9dc0-b4372f1a2718",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_customers_df = customers_df.sortWithinPartitions('customer_id')\n",
    "sorted_orders_df = complete_df.sortWithinPartitions('customer_id')\n",
    "\n",
    "optimized_complete_df = sorted_orders_df.join(sorted_customers_df,'customer_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62751b17-e2a5-4113-b4bd-239ef8597e51",
   "metadata": {},
   "source": [
    "### **Bucket Join**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e794db96-0219-4e6f-9f5b-e3a37685ed57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucketed_customers_df = customers_df.repartition(10, 'customer_id')\n",
    "bucketed_orders_df = complete_df.repartition(10, 'customer_id')\n",
    "\n",
    "bucketed_optimized_complete_df = sorted_orders_df.join(sorted_customers_df,'customer_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24178fe-00f4-4e34-9e14-58f6c23924b9",
   "metadata": {},
   "source": [
    "# **Saving and retreiving data efficiently inside and outside dataproc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8f12ca8-5323-4e82-ad51-e202300c459a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reading from parquet\n",
    "complete_df = spark.read.parquet('/data/olist/olist-merged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08241b3b-aac1-45ab-b9fb-2ab72b204e94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/25 23:50:42 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# saving as parquet in a GCS Bucket\n",
    "complete_df.write.mode('overwrite').parquet('gs://dataproc-staging-us-central1-1019955474270-diwuvdox/temp_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23b58fcf-a91a-4fdc-9dc0-67c8750c1f6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n",
      "25/05/25 23:52:29 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    }
   ],
   "source": [
    "# saving as a table which can be accessed in hive\n",
    "\n",
    "complete_df.write.mode('overwrite').saveAsTable('complete_df_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d08a4-cbf9-4c5e-84cd-57be30766231",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                        (0 + 8) / 10]\r"
     ]
    }
   ],
   "source": [
    "# saving as csv in our hadoop cluster\n",
    "complete_df.write.mode('overwrite').option('header', 'true').csv('/data/olist/olist-merged/csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
